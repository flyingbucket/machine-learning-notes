## 贝叶斯公式

$$
\begin{aligned}
P(B_i|A)&=\frac{P(B_i)P(A|B_i)}{P(A)}\\
		&=\frac{P(B_i)P(A|B_i)}{\sum_{j=1}^nP(B_j)P(A|B_j)}\\
\end{aligned}
$$
在机器学习中一般写作：
$$
P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}
$$
称$P(\theta)$为先验，$P(\theta|X)$为后验

## 基本概念
在二分类问题中，我们考虑一下几个概念：

- 两个类别$\omega_1,\omega_2$
- 先验概率$P(\omega_1),P(\omega_2)$ 
- 类条件概率密度$p(X|\omega_1),p(X|\omega_2)$
- 随机事件观察值，特征向量X
后验概率

- $P(\omega_1|X)=\frac{p(X|\omega_1)P(\omega)}{p(X)}$
- $P(\omega_2|X)=\frac{p(X|\omega_2)P(\omega)}{p(X)}$
- $P(X)=p(X|\omega_1)P(\omega_1)+p(X|\omega_2)P(\omega_2)$

*目标是找到最小错误概率的分类器*

## 决策规则
以下几类决策规则相互等价(以二分类为例)：

- 后验概率：$P(\omega_1|X)>P(\omega_2|X)$则分入第一类，否则第二类
- 后验概率分子：$P(X|\omega_1)P(\omega_1)>P(X|\omega_2)P(\omega_2)$则分入第一类，否则第二类
- 似然比: $I(X)=\frac{P(X|\omega_1)}{P(X|\omega_2)}$,$I(X)>\frac{P(\omega_2)}{P(\omega_1)}$则分入第一类，否则第二类
- 对数似然比: $h(X)=ln{I(X)}$,$h(X)>lnP(\omega_2)-lnP(\omega_1)$则分入第一类，否则第二类

## 最小错误概率

在同一分布下、同一个分类问题下，最优贝叶斯分类器给出了所有分类器可能达到的错误概率下界，
但想要达到这个下界，必须已知各个类别的先验概率和每个数据在各个类别下的条件概率，
如果没有这些信息，我们只能用各种其他办法来趋近这个下界。
这里要求已知的是所有数据在整个分布下的信息而不是数据集上的信息，因为数据集的抽样可能是有偏的。

错误概率如下定义:
考虑d维特征空间D,所有点来自m类$\{\omega_i\}_{i=1}^m$,记各个类别对应的特征空间区域为$\{D_i\}_{i=1}^m$，记$R_i=D\setminus D_I$,则总分类错误概率定义为
$$
P(e)=\sum_{i=1}^m\int_{R_i}P(\omega_i|X)P(X)dX
$$