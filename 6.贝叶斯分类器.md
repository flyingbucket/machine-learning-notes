## 贝叶斯公式

$$
\begin{aligned}
P(B_i|A)&=\frac{P(B_i)P(A|B_i)}{P(A)}\\
		&=\frac{P(B_i)P(A|B_i)}{\sum_{j=1}^nP(B_j)P(A|B_j)}\\
\end{aligned}
$$
在机器学习中一般写作：
$$
P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}
$$
称$P(\theta)$为先验，$P(\theta|X)$为后验

## 基本概念
在二分类问题中，我们考虑一下几个概念：
- 两个类别$\omega_1,\omega_2$
- 先验概率$P(\omega_1),P(\omega_2)$ 
- 类条件概率密度$p(X|\omega_1),p(X|\omega_2)$
- 随机事件观察值，特征向量X
后验概率
- $P(\omega_1|X)=\frac{p(X|\omega_1)P(\omega)}{p(X)}$
- $P(\omega_2|X)=\frac{p(X|\omega_2)P(\omega)}{p(X)}$
- $P(X)=p(X|\omega_1)P(\omega_1)+p(X|\omega_2)P(\omega_2)$

*目标是找到最小错误概率的分类器*

## 决策规则
以下几类决策规则相互等价：
- 后验概率：$P(\omega_1|X)>P(\omega_2|X)$则分入第一类，否则第二类
- 后验概率分子：$P(X|\omega_1)P(\omega_1)>P(X|\omega_2)P(\omega_2$则分入第一类，否则第二类

## 最小错误概率及证明

我们认为特征的任何一个维度对分类结果都不具有因果性的决定关系，只存在概率意义上的相关性，即任给特征空间中一点，该点都有概率被分到任何一类，从更大的角度说，各个类别真值作为随机变量，在整个特征空间上有密度函数，而不是仅在特征空间的某个区域内出现。

下面我在**你已有的表述风格与思想框架下**，把**贝叶斯分类器达到最小错误概率的严格数学证明**完整续写出来。情形限定为：**二分类 + 一维特征**，但推理本质上是逐点（pointwise）的，因此非常清晰。


在上述认识下，我们引入形式化描述。

设类别随机变量  
$$  
\omega \in {\omega_1,\omega_2},  
$$ 
其先验概率分别为  
$$ 
P(\omega_1)=\pi_1,\quad P(\omega_2)=\pi_2,\quad \pi_1+\pi_2=1.  
$$

设一维特征随机变量为 (X\in\mathbb{R})，其在给定类别条件下具有类条件概率密度函数  
$$  
f_i(x)=p(x\mid \omega_i),\quad i=1,2.  
$$

由全概率公式，特征的边缘密度为  
$$  
p(x)=\pi_1 f_1(x)+\pi_2 f_2(x).  
$$

---

### 一、分类规则与条件错误概率

任意一个分类器本质上对应于对特征空间的一个划分：  
$$  
\mathbb{R} = R_1 \cup R_2,\quad R_1\cap R_2=\varnothing,  
$$  
其中当 $x\in R_i$ 时，分类器判定 $x$ 属于 $\omega_i$。

对任意给定的 $x$，若分类器将其判为 $\omega_1$，则该点的**条件错误概率**为  
$$  
P(\text{error}\mid x, \text{decide }\omega_1)=P(\omega_2\mid x),  
$$  
同理，若判为 $\omega_2$，条件错误概率为  
$$  
P(\text{error}\mid x, \text{decide }\omega_2)=P(\omega_1\mid x).  
$$

因此，对固定的 $x$，为了使错误概率最小，显然应选择**后验概率较大的类别**。

---

### 二、Bayes 判决规则的点态最优性

由贝叶斯公式，  
$$  
P(\omega_i\mid x)=\frac{\pi_i f_i(x)}{p(x)},\quad i=1,2.  
$$

因此，对任意给定的 $x$，最优判决规则为  
$$  
\boxed{  
\text{若 }\pi_1 f_1(x)>\pi_2 f_2(x),\ \text{则判为 }\omega_1;  
\quad  
\text{否则判为 }\omega_2.  
}  
$$

该规则称为 **Bayes 分类器**。

注意这一结论是**逐点成立的**：

> 对特征空间中每一个点 $x$，Bayes 分类器在该点上使条件错误概率达到最小。

---

### 三、最小错误概率的表达式

在 Bayes 判决规则下，对给定的 $x$，错误概率为  
$$ 
\min\big(P(\omega_1\mid x),\ P(\omega_2\mid x)\big).  
$$

于是，总的分类错误概率为  
$$  
P_e  
= \int_{-\infty}^{\infty}  
\min\big(P(\omega_1\mid x),\ P(\omega_2\mid x)\big)\ p(x)\ dx.  
$$

将后验概率代入并消去 $p(x)$，得到经典形式：  
$$  
\boxed{  
P_e  
= \int_{-\infty}^{\infty}  
\min\big(\pi_1 f_1(x),\ \pi_2 f_2(x)\big)dx.  
}  
$$

这就是**Bayes 最小错误概率**。

---

### 四、最优性的严格性说明

由于：
- 任意分类器在每个 (x) 上只能选择一个类别；
- Bayes 分类器在每个 (x) 上都选择使条件错误概率最小的类别；

因此对任意其他分类规则，其在某些 (x) 上的条件错误概率必然不小于 Bayes 分类器对应的值。积分后即得：

$$  
P_e^{\text{any classifier}} \ge P_e^{\text{Bayes}}.  
$$

从而证明：  
$$  
\boxed{  
\text{Bayes 分类器在所有分类器中实现了最小可能的分类错误概率。}  
}  
$$

所谓“分类边界”，并不是类别的真实分界，而只是  
$$  
\pi_1 f_1(x) = \pi_2 f_2(x)  
$$ 
这一统计意义下的**等后验曲线**。

