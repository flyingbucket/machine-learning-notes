## 贝叶斯公式

$$
\begin{aligned}
P(B_i|A)&=\frac{P(B_i)P(A|B_i)}{P(A)}\\
		&=\frac{P(B_i)P(A|B_i)}{\sum_{j=1}^nP(B_j)P(A|B_j)}\\
\end{aligned}
$$
在机器学习中一般写作：
$$
P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}
$$
称$P(\theta)$为先验，$P(\theta|X)$为后验

## 基本概念
在二分类问题中，我们考虑一下几个概念：
- 两个类别$\omega_1,\omega_2$
- 先验概率$P(\omega_1),P(\omega_2)$ 
- 类条件概率密度$p(X|\omega_1),p(X|\omega_2)$
- 随机事件观察值，特征向量X
后验概率
- $P(\omega_1|X)=\frac{p(X|\omega_1)P(\omega)}{p(X)}$
- $P(\omega_2|X)=\frac{p(X|\omega_2)P(\omega)}{p(X)}$
- $P(X)=p(X|\omega_1)P(\omega_1)+p(X|\omega_2)P(\omega_2)$

*目标是找到最小错误概率的分类器*

## 决策规则
以下几类决策规则相互等价：
- 后验概率：$P(\omega_1|X)>P(\omega_2|X)$则分入第一类，否则第二类
- 后验概率分子：$P(X|\omega_1)P(\omega_1)>P(X|\omega_2)P(\omega_2$则分入第一类，否则第二类

## 最小错误概率及证明

我们认为特征的任何一个维度对分类结果都不具有因果性的决定关系，只存在概率意义上的相关性，即任给特征空间中一点，该点都有概率被分到任何一类，从更大的角度说，各个类别真值作为随机变量，在整个特征空间上有密度函数，而不是仅在特征空间的某个区域内出现。

下面我在**你已有的表述风格与思想框架下**，把**贝叶斯分类器达到最小错误概率的严格数学证明**完整续写出来。情形限定为：**二分类 + 一维特征**，但推理本质上是逐点（pointwise）的，因此非常清晰。


在上述认识下，我们引入形式化描述。

设类别随机变量  
$$  
\omega \in {\omega_1,\omega_2},  
$$ 
其先验概率分别为  
$$ 
P(\omega_1)=\pi_1,\quad P(\omega_2)=\pi_2,\quad \pi_1+\pi_2=1.  
$$

设一维特征随机变量为 (X\in\mathbb{R})，其在给定类别条件下具有类条件概率密度函数  
$$  
f_i(x)=p(x\mid \omega_i),\quad i=1,2.  
$$

由全概率公式，特征的边缘密度为  
$$  
p(x)=\pi_1 f_1(x)+\pi_2 f_2(x).  
$$

---

### 一、分类规则与条件错误概率

任意一个分类器本质上对应于对特征空间的一个划分：  
$$  
\mathbb{R} = R_1 \cup R_2,\quad R_1\cap R_2=\varnothing,  
$$  
其中当 $x\in R_i$ 时，分类器判定 $x$ 属于 $\omega_i$。

对任意给定的 $x$，若分类器将其判为 $\omega_1$，则该点的**条件错误概率**为  
$$  
P(\text{error}\mid x, \text{decide }\omega_1)=P(\omega_2\mid x),  
$$  
同理，若判为 $\omega_2$，条件错误概率为  
$$  
P(\text{error}\mid x, \text{decide }\omega_2)=P(\omega_1\mid x).  
$$

因此，对固定的 $x$，为了使错误概率最小，显然应选择**后验概率较大的类别**。

---

### 二、Bayes 判决规则的点态最优性

由贝叶斯公式，  
$$  
P(\omega_i\mid x)=\frac{\pi_i f_i(x)}{p(x)},\quad i=1,2.  
$$

因此，对任意给定的 $x$，最优判决规则为  
$$  
\boxed{  
\text{若 }\pi_1 f_1(x)>\pi_2 f_2(x),\ \text{则判为 }\omega_1;  
\quad  
\text{否则判为 }\omega_2.  
}  
$$

该规则称为 **Bayes 分类器**。

注意这一结论是**逐点成立的**：

> 对特征空间中每一个点 $x$，Bayes 分类器在该点上使条件错误概率达到最小。

---

### 三、最小错误概率的表达式

在 Bayes 判决规则下，对给定的 $x$，错误概率为  
$$ 
\min\big(P(\omega_1\mid x),\ P(\omega_2\mid x)\big).  
$$

于是，总的分类错误概率为  
$$  
P_e  
= \int_{-\infty}^{\infty}  
\min\big(P(\omega_1\mid x),\ P(\omega_2\mid x)\big)\ p(x)\ dx.  
$$

将后验概率代入并消去 $p(x)$，得到经典形式：  
$$  
\boxed{  
P_e  
= \int_{-\infty}^{\infty}  
\min\big(\pi_1 f_1(x),\ \pi_2 f_2(x)\big)dx.  
}  
$$

这就是**Bayes 最小错误概率**。

---

### 四、最优性的严格性说明

由于：
- 任意分类器在每个 (x) 上只能选择一个类别；
- Bayes 分类器在每个 (x) 上都选择使条件错误概率最小的类别；

因此对任意其他分类规则，其在某些 (x) 上的条件错误概率必然不小于 Bayes 分类器对应的值。积分后即得：

$$  
P_e^{\text{any classifier}} \ge P_e^{\text{Bayes}}.  
$$

从而证明：  
$$  
\boxed{  
\text{Bayes 分类器在所有分类器中实现了最小可能的分类错误概率。}  
}  
$$

所谓“分类边界”，并不是类别的真实分界，而只是  
$$  
\pi_1 f_1(x) = \pi_2 f_2(x)  
$$ 
这一统计意义下的**等后验曲线**。


## 最小风险分类器
对于k类别的分类问it
定义分类损失(风险)$L(C_i,C_j),i,j=1\cdots k$ ,表示实际类别为$C_j$时错分为$C_i$,导致的损失
期望风险是对不同分类决策导致的分类损失的加权平均
$$
R(C_i|x)=\sum_{i=1}^k L(C_i,C_j)P(C_j|x)
$$
决策规则：选择使得期望风险最小化的类别作为分类输出结果$\arg \min_{C_i} R(C_i|x)$

## 正态分布决策面
### 正态假设
在k分类问题中，假设每个类别的条件概率服从正态: 
$$
P(x|C_i)=\Phi(\mu_i,\sigma_i^2)=\frac{1}{(2\pi)^{D/2}|\Sigma_i|^{1/2}}exp\{-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)\}
$$
决策规则为选择后验概率最大的类别作为输出判断：
$$
\begin{aligned}
\arg \max_{C_i} P(x|C_i)P(C_i) 
&= \arg \max_{C_i}P(x|C_i)P(C_i)=\Phi(\mu_i,\sigma^2_i)P(C_i)\\
&= \arg \max_{C_i}[-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)+logP(C_i)]\\
\end{aligned}
$$
### 决策面(以二分类为例)
决策面是指把样本空间划分为不同类别的边界。在二分类问题中，假设类别 $C_1$ 和 $C_2$ 的特征 $x$ 满足正态分布，则决策边界的计算如下：

$$  
P(x | C_1) P(C_1) = P(x | C_2) P(C_2)  
$$

取对数并简化后得到决策面方程：

$$ 
-\frac{1}{2}(x - \mu_1)^T\Sigma_1^{-1}(x-\mu_1) + \log P(C_1) = -\frac{1}{2}(x - \mu_2)^T\Sigma_2^{-1}(x-\mu_2) + \log P(C_2) 
$$
这时，决策面是由上述方程描述的，它的形状通常是一个**二次曲线**（对于一维情况是一个抛物线，对于高维空间则是一个高维的二次面）。

### 决策面特例
#### 一、
$$
\Sigma=\sigma^2I
$$
此时决策规则可以简化为
$$
\arg \max_{C_i}[-\frac{1}{2}(x-\mu_i)^T(x-\mu_i)+logP(C_i)]
$$
决策面为
$$
(x-\mu_1)^T(x-\mu_1)-(x-\mu_2)^T(x-\mu_2)=log\frac{P(C_2)}{P(C_1)}
$$
在正态假设和特征各维度无关的假设下，LDA分类器是一个错误率达到贝叶斯分类器错误率下界的分类器。

#### 二、

我们有 $k$ 个类别 $C_1, C_2, \dots, C_k$，每个类别$C_i$的特征$x$ 服从多元正态分布，且它们有相同的协方差矩阵 $\Sigma$，即：

$$
P(x | C_i) = \frac{1}{(2 \pi)^{d/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (x - \mu_i)^T \Sigma^{-1} (x - \mu_i) \right)
$$

其中：
- $mu_i$ 是类别$C_i$的均值向量。
- $\Sigma$ 是所有类别共享的协方差矩阵。

贝叶斯分类器的目标是通过最大化后验概率 $P(C_i | x)$ 来进行分类。根据贝叶斯定理：

$$
P(C_i | x) = \frac{P(x | C_i) P(C_i)}{P(x)}
$$

在此情况下，$P(x)$ 是常数，因此贝叶斯分类器的决策规则简化为选择使得 $P(x | C_i) P(C_i)$ 最大的类别 $C_i$。

对于每个类别 $C_i$，其条件概率 $P(x | C_i)$是正态分布，可以写为：

$$
P(x | C_i) = \frac{1}{(2 \pi)^{d/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (x - \mu_i)^T \Sigma^{-1} (x - \mu_i) \right)
$$

将其代入贝叶斯分类器的决策规则，得到：

$$
\hat{C}(x) = \arg \max_{C_i} \left[ -\frac{1}{2} (x - \mu_i)^T \Sigma^{-1} (x - \mu_i) + \log P(C_i) \right]
$$

我们对决策规则取对数，得到：

$$
\hat{C}(x) = \arg \max_{C_i} \left[ -\frac{1}{2} (x - \mu_i)^T \Sigma^{-1} (x - \mu_i) + \log P(C_i) \right]
$$

简化后，我们得到：

$$
\hat{C}(x) = \arg \max_{C_i} \left[ -\frac{1}{2} \left( x^T \Sigma^{-1} x - 2 x^T \Sigma^{-1} \mu_i + \mu_i^T \Sigma^{-1} \mu_i \right) + \log P(C_i) \right]
$$

对比不同类别的条件概率时，二次项 $x^T \Sigma^{-1} x$ 是常数，因此可以忽略。我们得到：

$$
\hat{C}(x) = \arg \max_{C_i} \left[ -\left( - x^T \Sigma^{-1} \mu_i + \mu_i^T \Sigma^{-1} \mu_i \right) + \log P(C_i) \right]
$$

$$
\hat{C}(x) = \arg \max_{C_i} \left[ x^T \Sigma^{-1} \mu_i - \frac{1}{2} \mu_i^T \Sigma^{-1} \mu_i + \log P(C_i) \right]
$$

现在，我们要求使得两个类别 $C_1$ 和 $C_2$ 的后验概率相等，即：
$$
P(x | C_1) P(C_1) = P(x | C_2) P(C_2)
$$
代入公式，得到：
$$
x^T \Sigma^{-1} \mu_1 - \frac{1}{2} \mu_1^T \Sigma^{-1} \mu_1 + \log P(C_1) = x^T \Sigma^{-1} \mu_2 - \frac{1}{2} \mu_2^T \Sigma^{-1} \mu_2 + \log P(C_2)
$$
整理得到：

$$
x^T \Sigma^{-1} (\mu_1 - \mu_2) = \frac{1}{2} \left( \mu_1^T \Sigma^{-1} \mu_1 - \mu_2^T \Sigma^{-1} \mu_2 \right) + \log \frac{P(C_2)}{P(C_1)}
$$

这就是决策面的方程。可以看到，决策面是一个**超平面**，由下面的线性方程描述：

$$
x^T \Sigma^{-1} (\mu_1 - \mu_2) = \text{常数}
$$

当所有类别的协方差矩阵相同且为 $\Sigma$时，贝叶斯分类器的决策面是一个**线性超平面**。这个超平面是由类别均值之间的差异和协方差矩阵的逆构成的，表示不同类别之间的分界线。决策面的位置由类别均值$\mu_1$ 和 $\mu_2$ 之间的差异以及先验概率 $P(C_1)$ 和$P(C_2)$的比值决定。
