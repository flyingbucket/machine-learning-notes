## 基本概念

## 特征提取
在统计中降维往往以保持原有信息为目的，而机器学习的降维是和人物高度相关的，不一定要保持悠悠的信息。

### PCA
给定d个原始特征，经过正交线性变换得到一组按照“重要性”大小排序的特征，取前m个特征。
其中重要性定义为数据在该维度上的方差，方差越大越重要。

假设原始特征矩阵为$X^{n \times d}$,其中n为样本数、d为原始数据的维度. 
关键是找到一组投影向量，使得原始数据在这些方向上的投影的方差比较大，投影方向上数据方差越大说明该方向越重要，把所有的投影方向按照数据投影的方差大小排序，选出前m个方向向量就得到了这组数据的主成分。

中心化：$\tilde{X}=X-\bar{X}$
把每个特征当成一个随机变量，计算特征之间的协方差矩阵$\Sigma = \frac{1}{n-1}\tilde{X}^T\tilde{X}$
*除以n-1计算的是样本协方差，保证无偏估计*

对任意单位方向向量$\omega$ ,数据在该方向上的投影的一写成$z_i = \omega^T \tilde{X}$ 
#### 在该方向上的样本方差可以写为
$$
\begin{aligned}
Var(z) & = \frac{1}{n-1} \sum_i(z_i-\bar{z})^2\\
       & = \frac{1}{n-1} \sum_iz_i^2 \\
       & = \frac{1}{n-1} \sum_i(\omega^T\tilde{x_i})^2 \\
       & = \omega^T(\frac{1}{n-1}\tilde{X}^T\tilde{X})\omega \\
       & = \omega^T\Sigma\omega
\end{aligned}
$$
首先要找PCA的第一个维度及方差最大的方向向量，这就转化为一个约束优化问题：
$$
	\max_\omega \ \omega^T\Sigma\omega \quad s.t.\quad \omega^T\omega=1
$$
#### 拉格朗日乘子法求解
使用拉格朗日乘子法：
$$
$$
$$
\begin{aligned}
L(\omega,\lambda) &= \omega^T\Sigma\omega-\lambda\omega^T\omega \\
\\
\frac{\partial{L}}{\partial{\omega}} 
&= 2\Sigma\omega-2\lambda\omega \\
&= 0 \\
\end{aligned}
$$
$\Sigma\omega-\lambda\omega=0$求解这个方程就是求$\Sigma$的特征值和特征向量。
#### 特征值与投影方差的关系
特征值等于投影方向的方差，特征值越大方差越大。
按照特征值的大小排序，可以得到m个主成分向量（投影方向）。

### LDA 

LDA是有监督的降维方法，每个数据都有标签，降维的目的是把原始特征映射到低维度空间，使得低维度空间中不同类别的数据距离尽可能远，相同类别数据尽可能近。
假设数据有c个类别,第i类共有$n_i$个样本:$X_i=\{x_1^{(i)},\cdots,x_{n_i}^{(i)}\}$
类内均值为$\mu_i=\frac{1}{n_i}\sum_{x\in X_i}x$
总体均值为$\mu=\frac{1}{c}\sum_{i=1}^c\mu_i$
用协方差矩阵定义类内距离和类间距离
在原始空间中定义类内距离和类间距离矩阵：
$$
\begin{aligned}
\text{类内距离}\quad S_W &=\sum_{i=1}^c\sum_{x\in X_i}(x-\mu_i)(x-\mu_i)^T \\
\text{类间距离}\quad S_B &=\sum_{i=1}^c n_i(\mu_i-\mu)(\mu_i-\mu)^T
\end{aligned}
$$

设降维到k维，投影矩阵为$W^{d\times k}$,$y=W^Tx$
则在投影空间中距离为：
$$
\begin{aligned}
\text{类内距离}\quad S_W' &=W^TS_WW \\
\text{类间距离}\quad S_B' &=W^TS_BW
\end{aligned}
$$
找到最优投影矩阵W使得Fisher函数最大化
$$
J(W)=\frac{tr(S_W')}{tr(S_B')}
$$

优化问题可以转化为：
$$
min -tr(W^TS_WW) \quad s.t. \quad tr(W^TS_BW)=1
$$
使用拉格朗日乘子法进行优化。

*fisher投影可以看作LDA的一个特例*

## 特征选择

特征选择是另一种降维方法，从所有特征中选出一个子集用于相关机器学习任务。
