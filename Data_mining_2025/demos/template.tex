\documentclass[a4paper,12pt]{report}
\usepackage[margin=1in]{geometry} % to change the page dimensions
\usepackage{ctex}
\usepackage{xeCJK}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{times}
\usepackage{setspace}
% \usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{graphicx}
%\graphicspath{{fig/}}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{array}  
% \usepackage{fontspec,xunicode,xltxtra}
% \renewcommand{\sfdefault}{cmr}
\usepackage{titlesec}
\usepackage{titletoc}
\usepackage[titletoc]{appendix}
%\usepackage[top=30mm,bottom=30mm,left=20mm,right=20mm]{geometry}
%\usepackage{cite}
\usepackage[backend = biber, style = gb7714-2015, defernumbers=true]{biblatex}
\renewcommand*{\bibfont}{\small}
\addbibresource{reference.bib}
%\usepackage{courier}
\setmonofont{Courier New}
\usepackage{listings}
\lstset{tabsize=4, keepspaces=true,
    xleftmargin=2em,xrightmargin=0em, aboveskip=1em,
    %backgroundcolor=\color{gray!20},  % 定义背景颜色
    frame=none,                       % 表示不要边框
    extendedchars=false,              % 解决代码跨页时，章节标题，页眉等汉字不显示的问题
    numberstyle=\ttfamily,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue}\bfseries,
    breakindent=10pt,
    identifierstyle=,                 % nothing happens
    commentstyle=\color{green}\small,  % 注释的设置
    morecomment=[l][\color{green}]{\#},
    numbers=left,stepnumber=1,numberstyle=\scriptsize,
    showstringspaces=false,
    showspaces=false,
    flexiblecolumns=true,
    breaklines=true, breakautoindent=true,breakindent=4em,
    escapeinside={/*@}{@*/},
}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{定理}
\newtheorem{definition}{定义}
\newtheorem{corollary}{推论}
\newtheorem{example}{例}
\usepackage{amsfonts}
%\usepackage{bm}
\usepackage{booktabs} % for much better looking tables
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfigure} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{cases} %equation set
\usepackage{multirow} %use table
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=black,anchorcolor=black,citecolor=black, pdfstartview=FitH,bookmarksnumbered=true,bookmarksopen=true,} % set href in tex & pdf
%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode} % 插入matlab代码
\XeTeXlinebreaklocale "zh"
\XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt

\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother
%%%%此处break 算法
%---------------------------------------------------------------------
%	页眉页脚设置
%---------------------------------------------------------------------
\fancypagestyle{plain}{
    \pagestyle{fancy}      %改变章节首页页眉
}

\pagestyle{fancy}
\lhead{\kaishu~数据挖掘实验报告~}
%\rhead{\kaishu~xxx}
\cfoot{\thepage}
\titleformat{\chapter}{\centering\zihao{2}\heiti}{第\chinese{chapter}章}{1em}{}
% \titleformat{\chapter*}{\centering\zihao{-1}\heiti}
\begin{comment}
%---------------------------------------------------------------------
%	章节标题设置
%---------------------------------------------------------------------
\titleformat{\chapter}{\centering\zihao{-1}\heiti}{实验\chinese{chapter}}{1em}{}
\titlespacing{\chapter}{0pt}{*0}{*6}
\end{comment}
%---------------------------------------------------------------------
%	摘要标题设置
%---------------------------------------------------------------------
%\renewcommand{\abstractname}{摘要}
\renewcommand{\figurename}{图}
\renewcommand{\tablename}{表}

%---------------------------------------------------------------------
%	参考文献设置
%---------------------------------------------------------------------
%\renewcommand{\bibname}{\zihao{2}{\hspace{\fill}参\hspace{0.5em}考\hspace{0.5em}文\hspace{0.5em}献\hspace{\fill}}}
\renewcommand{\bibname}{参考文献}
\begin{comment}
%---------------------------------------------------------------------
%	引用文献设置为上标
%---------------------------------------------------------------------
\makeatletter
\def\@cite#1#2{\textsuperscript{[{#1\if@tempswa , #2\fi}]}}
\makeatother
\end{comment}
%---------------------------------------------------------------------
%	目录页设置
%---------------------------------------------------------------------
%\renewcommand{\contentsname}{\zihao{-3} 目\quad 录}
\renewcommand{\contentsname}{目录}
\titlecontents{chapter}[0em]{\songti\zihao{-4}}{\thecontentslabel\ }{}
{\hspace{.5em}\titlerule*[4pt]{$\cdot$}\contentspage}
\titlecontents{section}[2em]{\vspace{0.1\baselineskip}\songti\zihao{-4}}{\thecontentslabel\ }{}
{\hspace{.5em}\titlerule*[4pt]{$\cdot$}\contentspage}
\titlecontents{subsection}[4em]{\vspace{0.1\baselineskip}\songti\zihao{-4}}{\thecontentslabel\ }{}
{\hspace{.5em}\titlerule*[4pt]{$\cdot$}\contentspage}

\begin{document}
%---------------------------------------------------------------------
%	封面设置
%---------------------------------------------------------------------
\begin{titlepage}
    \begin{center}
        
    \includegraphics[width=0.60\textwidth]{nk_logo.pdf}\\
    \vspace{10mm}
    \hspace*{\fill} \\
    \textbf{\zihao{1}{数据挖掘实验报告}}\\
    \vspace{\fill}
    
\setlength{\extrarowheight}{3mm}
{\songti\zihao{3}	
\begin{tabular}{rl}
    
    {\makebox[4\ccwd][s]{学\qquad 号：}} & ~\kaishu   \\
    {\makebox[4\ccwd][s]{姓\qquad 名：}} & ~\kaishu   \\
    {\makebox[4\ccwd][s]{年\qquad 级：}} & ~\kaishu   \\
    {\makebox[4\ccwd][s]{学\qquad 院：}} & ~\kaishu   \\
    {\makebox[4\ccwd][s]{专\qquad 业：}} & ~\kaishu   \\
    %{\makebox[4\ccwd][s]{授课教师：}}  & ~\kaishu xxx~教授\\ 
    %{\makebox[4\ccwd][s]{课程助教：}} & ~\kaishu xxx~xxx \\
    {\makebox[4\ccwd][s]{完成日期：}}  & ~\kaishu 2021年3月27日\\ 

\end{tabular}
 }\\[2cm]
%\vspace{\fill}
%\zihao{4}
%使用\LaTeX 撰写于\today
    \end{center}	
\end{titlepage}

%---------------------------------------------------------------------
%  摘要页
%---------------------------------------------------------------------


%---------------------------------------------------------------------
%  目录页
%---------------------------------------------------------------------
\tableofcontents % 生成目录

%---------------------------------------------------------------------
%  绪论
%---------------------------------------------------------------------
\chapter{第一次上机实验（LBP提取图像的纹理特征）}
\section{实验要求}
\begin{itemize}
\item{1. 给定若干张图像，利用局部二值模式特征(LBP)对这些图像进行特征提取}
\item{2. 图象是W * H * 3的矩阵}
\item{3. 将最终提取到的特征通过plot的形式展示，绘制特征曲线图直观对比不同类图片纹理提取到的特征的不同}
\item{4. 使用Python编程实现}
\end{itemize}
% \section{数据分析与处理}
% \par 没有可以不写，比如垂直平分分类器中
\section{实验步骤与原理}
\subsection{LBP 特征的基本定义}
局部二值模式（Local Binary Pattern, LBP）通过比较像素与其邻域像素的灰度关系，
编码局部纹理的微结构。给定中心像素 $g_c$ 及以其为中心、
半径为 $R$ 的圆形邻域上 $P$ 个等角度采样点的灰度 
$\{g_p\}_{p=0}^{P-1}$，标准 LBP 的定义为
\[
\mathrm{LBP}_{P,R}(x_c,y_c)
=\sum_{p=0}^{P-1} s(g_p-g_c)\,2^{p},\qquad
s(t)=\begin{cases}
1,& t\ge 0,\\
0,& t<0,
\end{cases}
\]
其中邻域采样点坐标为
\[
(x_p, y_p) = \bigl(x_c + R\cos(2\pi p/P),\; y_c - R\sin(2\pi p/P)\bigr),
\]
本次实验只考虑以$g_c$为中心的九宫格的局部的LBP特征。

\subsection{直方图特征}
将整幅图像（或图像块）内的 LBP 代码统计为直方图作为纹理特征：
\[
H[k]=\sum_{(x,y)} \mathbf{1}\{\mathrm{LBP}_{P,R}(x,y)=k\},\qquad k\in\{0,\dots,2^P-1\}.
\]
常见做法是对直方图进行 $\ell_1$ 归一化以消除尺寸影响：
\[
\hat H[k]=\frac{H[k]}{\sum_{j} H[j]}.
\]
为表征空间布局，可将图像划分为 $M\times N$ 个网格单元，分别计算直方图并按行优先串接，得到最终特征向量。


\subsection{实现细节（本实验手写 Python 要点）}
\begin{enumerate}
  \item \textbf{预处理：} 彩色图像先转灰度；可选高斯平滑抑制噪声。
  \item{按上述规则计算出图片的LBP特征直方图}
  \item \textbf{可视化：} 使用 Matplotlib 绘制折线；多类对比时可叠加均值曲线与标准差带。
\end{enumerate}

\subsection{复杂度与并行优化}
\begin{itemize}
  \item{时间复杂度约为 $O(PWH)$，$W,H$ 为图像宽高；$P$ 通常较小，易于并行/向量化。}
  \item{下面所呈现的代码采用串行方式计算LBP特征，但本人也给出了基于cython的并行加速版本。}
\end{itemize}

\noindent\textbf{加速计算技巧：}
\begin{itemize}
  \item{使用cython 的memoryview接口直接操作numpy.ndarray}
  \item{在cython层开启python的nogil模式，绕开全局解释器锁，使用OpenMP实现并行计算}
\end{itemize}

并行计算代码及各类计算方法的benchmark详见

\url{https://github.com/flyingbucket/machinelearning/tree/main/LBP}。
\section{实验结果与分析}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{../images/cloud_lbp_curves.png}
    \caption{cloud LBP 特征曲线对比图}
    \label{fig:cloud_lbp_curve}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{../images/forest_lbp_curves.png}
    \caption{forestLBP 特征曲线对比图}
    \label{forest_lbp_curve}
\end{figure}

\section{实验代码}
\begin{lstlisting}[language=Python]
  import numpy as np
  from matplotlib import pyplot as plt
  from collections import Counter
  from PIL import Image

  class LBP:
      @staticmethod
      def _read_img(imPath: str, pad: int = 1, mode: str = "reflect") -> np.ndarray:
          im = Image.open(imPath).convert("L")
          arr = np.array(im)
          padded = np.pad(arr, pad_width=((pad, pad), (pad, pad)), mode=mode)
          return padded

      @staticmethod
      def LBPkernel(im: np.ndarray, x, y) -> int:
          h, w = im.shape
          assert x + 2 < h and y + 2 < w, (
              f"Index out of bound,please check padding. x:{x},y:{y},h:{h},w:{w}"
          )
          patch = im[x : x + 3, y : y + 3].copy()
          patch = (patch >= patch[1, 1]).astype(np.uint8)
          idxs = [0, 1, 2, 5, 8, 7, 6, 3]
          bits = patch.reshape(-1)[idxs]
          val = int("".join(map(str, bits)), 2)
          return val

  def walk_dir(root_dir: str, out_dir: str = "EX1/outputs"):
      root = Path(root_dir)
      out = Path(out_dir)
      out.mkdir(parents=True, exist_ok=True)
      LBPcyExecutor = LBP()

      for class_dir in sorted([p for p in root.iterdir() if p.is_dir()]):
          hist_list = []
          img_names = []
          all_codes = set()
          for img_path in sorted(class_dir.iterdir()):
              try:
                  res_dict = LBPcyExecutor(str(img_path))  # {code: count}
                  if not isinstance(res_dict, dict) or len(res_dict) == 0:
                      print(f"[WARN] 空直方图：{img_path}")
                      continue
                  hist_list.append(res_dict)
                  img_names.append(img_path.stem)
                  all_codes.update(res_dict.keys())
              except Exception as e:
                  print(f"[WARN] 处理失败: {img_path} -> {e}")

          codes = sorted(all_codes)  # 所有出现过的 LBP code
          X = []  # 每张图对齐后的频率向量

          for h in hist_list:
              vec = np.array([h.get(c, 0) for c in codes], dtype=np.float64)
              X.append(vec)

          plt.figure(figsize=(10, 6))
          for vec, name in zip(X, img_names):
              plt.plot(codes, vec, linewidth=1.2, alpha=0.85, label=name)
          plt.xlabel("LBP code")
          plt.ylabel("count")
          plt.title(f"LBP feature curves - {class_dir.name}")
          plt.legend(ncol=2, fontsize=9, loc="best")
          plt.tight_layout()

          save_path = out / f"{class_dir.name}_lbp_curves.png"
          plt.savefig(save_path, dpi=160)
          plt.close()
          print(f"[OK] Saved: {save_path}")

  if __name__ == "__main__":
      dir = "./EX1/data"
      walk_dir(dir)

\end{lstlisting}
\clearpage
\chapter{第二次上机实验}
\section{实验要求}
\begin{itemize}
  \item 1. 根据分类结果（result.csv）绘制PR曲线
  \item 2. 使用Python编程实现
\end{itemize}
\section{数据分析与处理}

\textbf{数据分析}

数据给出了分类器在测试集上的推理结果，包含两列，lable和pred

\textbf{数据处理}

将数据按照预测值递减排序
\section{实验步骤与原理}

\subsection{原理说明}

在二分类任务中，分类器的输出通常为一个介于 $[0,1]$ 之间的预测概率或置信度分数。通过设定不同的阈值（Threshold），可以将样本划分为正类或负类，从而得到不同的分类结果。

针对每一个阈值 $\theta$，可计算以下指标：
\begin{itemize}
  \item \textbf{真正例（TP）}：预测为正类且实际为正类的样本数；
  \item \textbf{假正例（FP）}：预测为正类但实际为负类的样本数；
  \item \textbf{假负例（FN）}：预测为负类但实际为正类的样本数；
  \item \textbf{真负例（TN）}：预测为负类且实际为负类的样本数。
\end{itemize}

由此可计算两个关键性能指标：
\[
\text{Precision} = \frac{TP}{TP + FP}, \qquad
\text{Recall} = \frac{TP}{TP + FN}
\]

当阈值从 1 逐渐减小到 0 时，Recall 通常单调递增，而 Precision 可能上升或下降。将各个阈值对应的 $(\text{Recall}, \text{Precision})$ 点连接起来，即得到 \textbf{Precision–Recall (PR) 曲线}。

PR 曲线反映了模型在不同阈值下的精确率与召回率的权衡关系，常用于评估类别分布不平衡的分类任务。曲线下的面积（AUC-PR）越大，说明模型整体性能越优。

\subsection{实验步骤}

\begin{enumerate}
  \item \textbf{数据读取与排序}：使用 \texttt{pandas} 读取 \texttt{result.csv} 文件，并按照预测值 \texttt{pred} 从大到小排序；
  \item \textbf{计算累计统计量}：
    \begin{itemize}
      \item 通过布尔判断 \texttt{(label == 1)} 计算真正例的累计和（\texttt{tp\_cumsum}）；
      \item 通过 \texttt{(label == 0)} 计算假正例的累计和（\texttt{fp\_cumsum}）；
    \end{itemize}
  \item \textbf{计算 Precision 与 Recall}：
    \[
    \text{Precision}_i = \frac{\text{TP}_i}{\text{TP}_i + \text{FP}_i}, \qquad
    \text{Recall}_i = \frac{\text{TP}_i}{\text{TotalPos}}
    \]
    其中 \texttt{TotalPos} 为真实正样本总数。
  \item \textbf{绘制 PR 曲线}：
    使用 \texttt{matplotlib} 将 Recall 作为横轴，Precision 作为纵轴，绘制曲线图；
  \item \textbf{性能评估（可选）}：
    计算 PR 曲线下的面积（AUC–PR），作为模型整体性能指标。
\end{enumerate}

\subsection{实验意义}

通过本实验，掌握了从分类结果计算 Precision–Recall 曲线的完整流程，理解了模型阈值调整对分类性能的影响，并熟悉了使用 Python 对实验结果进行可视化的基本方法。

\section{实验结论与分析}
根据给出的示例数据绘制出的PR曲线如图\ref{fig:PRcurve}所示：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{../images/PR_curve.png}
    \caption{示例数据的PR曲线}
    \label{fig:PRcurve}
\end{figure}
\section{实验代码}
\begin{lstlisting}[language=Python]
  import pandas as pd
  import numpy as np
  from matplotlib import pyplot as plt

  data = pd.read_csv("./EX2/data/result.csv").sort_values(by="pred",ascending=False).reset_index(drop=True)

  data["tp_cumsum"] = (data["label"] == 1).cumsum()
  data["fp_cumsum"] = (data["label"] == 0).cumsum()

  total_pos = (data["label"] == 1).sum()

  data["precision"] = data["tp_cumsum"] / (data["tp_cumsum"] + data["fp_cumsum"])
  data["recall"] = data["tp_cumsum"] / total_pos

  recall = np.r_[0.0, data["recall"].to_numpy()]
  precision = np.r_[1.0, data["precision"].to_numpy()]

# 计算 AUPR（recall 单调增时可用梯形法则）
  aupr = np.trapz(precision, recall)

  plt.figure()
  plt.step(recall, precision, where="post")
  plt.xlabel("Recall")
  plt.ylabel("Precision")
  plt.title(f"Precision–Recall Curve (AUPR = {aupr:.4f})")
  plt.xlim(0, 1)
  plt.ylim(0, 1)
  plt.grid(True)
  plt.savefig("./EX2/PR_curve.png")
  plt.show()
\end{lstlisting}
\clearpage
\chapter{第三次上机实验}
\section{实验要求}

1. 使用给定sklearn库中内置乳腺癌数据集，学习得到一个Fisher分类器
2. 对测试样本进行分类
3. 使用Python编程实现

\section{数据分析与处理}

本实验使用的是 \texttt{scikit-learn} 库提供的乳腺癌数据集。
该数据集包含 569 个样本、30 个连续特征以及一个二分类标签（0 表示恶性，1 表示良性），
数据字典中"data"和"target"分别给出了特征和对应的分类标签。

将数据集按 8:2 的比例随机划分为训练集和测试集，

\section{实验步骤与原理}

本实验旨在基于乳腺癌数据集构建 Fisher 线性判别分类器，
并对测试样本进行二分类评估。
Fisher 判别的核心思想是寻找一个投影方向，
使得不同类别在该方向上的类间距离最大、类内距离最小，从而实现最优线性可分。

\subsection{实验原理}

设原始数据为特征矩阵 $\mathbf{X}\in\mathbb{R}^{n\times d}$，标签向量为 $y$。
Fisher 判别通过构造两类散度矩阵来刻画数据结构：

\begin{itemize}
    \item \textbf{类内散度矩阵（Within-Class Scatter）}
    \[
        \mathbf{S}_w=\sum_{k}\sum_{x_i\in C_k}(x_i-\mu_k)(x_i-\mu_k)^{T},
    \]
    其中 $\mu_k$ 为第 $k$ 类的均值。该矩阵衡量同一类别样本的紧凑程度。
    
    \item \textbf{类间散度矩阵（Between-Class Scatter）}
    \[
        \mathbf{S}_b=\sum_{k}n_k(\mu_k-\mu)(\mu_k-\mu)^{T},
    \]
    其中 $\mu$ 为全局均值，$n_k$ 为类别 $k$ 的样本数。该矩阵表示类别均值之间的分离程度。
\end{itemize}

Fisher 判别寻求一个最优投影方向 $w$，使得类间散度与类内散度之比最大，即：
\[
    w=\arg\max_{w}\frac{w^{T}\mathbf{S}_b w}{w^{T}\mathbf{S}_w w}.
\]
这一优化问题,利用拉格朗日乘子法，可转化为广义特征值问题：
\[
    \mathbf{S}_b w = \lambda \mathbf{S}_w w,
\]
从中选取对应最大特征值的特征向量作为最优判别方向。

得到投影方向后，将训练样本投影到该方向，并计算各类别投影均值。类别之间的阈值取为相邻类别投影均值的中点，随后根据投影大小实现分类。

\subsection{实验步骤}

实验步骤如下：

\begin{enumerate}
    \item \textbf{数据加载与标注}：导入乳腺癌数据集，获取特征矩阵与标签向量。
    
    \item \textbf{数据划分}：采用随机划分方式，将数据按 8:2 的比例分为训练集与测试集。
    
    \item \textbf{计算散度矩阵}：对训练集分别计算类内散度矩阵 $\mathbf{S}_w$ 和类间散度矩阵 $\mathbf{S}_b$。
    
    \item \textbf{求解最优判别方向}：通过广义特征分解求得 Fisher 判别方向 $w$。
    
    \item \textbf{生成分类阈值}：将训练样本投影到 $w$ 上，根据各类别投影均值计算分类阈值。
    
    \item \textbf{模型预测}：将测试集样本投影到 $w$ 上，依据阈值进行类别判别。
    
    \item \textbf{模型评估}：计算预测精度，评价 Fisher 分类器在测试集上的分类性能。
\end{enumerate}

该流程完整实现了 Fisher 线性判别的训练与预测过程，验证了其在二分类任务中的有效性。

\section{实验结论与分析}
在完成 Fisher 线性判别分类器的训练与测试后，对测试集样本进行了分类评估。
本实验采用准确率（Accuracy）与混淆矩阵（Confusion Matrix）作为主要性能指标。

\subsection{分类准确率}
在固定numpy随机种子为42的情况下，得到如下结果。

分类准确率为：
\[
\text{Accuracy} = 97.37\%
\]
该结果表明 Fisher 判别在乳腺癌二分类任务中具有良好的判别能力，能够在一维投影空间中实现有效的类别分离。

\subsection{混淆矩阵分析}

为了进一步观察分类器在不同类别上的预测情况，绘制其混淆矩阵如下：

\[
\begin{pmatrix}
44 & 3 \\
0 & 67
\end{pmatrix}
\]

其中：

\begin{itemize}
    \item 第一行表示真实标签为恶性（0）的样本，共 47 个，其中正确分类 44 个，误判为良性 3 个；
    \item 第二行表示真实标签为良性（1）的样本，共 67 个，全部被正确分类。
\end{itemize}

从矩阵可以看出，分类器对良性样本的识别效果好（无误判），对恶性样本也能保持较高的识别率。整体来看，该 Fisher 分类器在测试集上表现稳定，具有较强的泛化能力。
但是存在将恶性样本误判为良性样本的情况，从错误代价的角度考虑，仍有较大的改进空间.

\section{实验代码}
\begin{lstlisting}[language=Python]

"""Fisher's Linear Discriminant Classifier Implementation"""

import scipy
import numpy as np
from sklearn.datasets import load_breast_cancer
from typing import List,Tuple

def get_Sw(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    """Coumpute Within-Class Scatter Matrix"""
    labels = np.unique(y)
    n_features = X.shape[1]
    Sw = np.zeros((n_features, n_features))
    for label in labels:
        XOfLabel = X[y == label]
        meanOfClass = np.mean(XOfLabel, axis=0)
        diff = XOfLabel - meanOfClass
        Sw += diff.T @ diff
    return Sw


def get_Sb(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    """Compute Between-Class Scatter Matrix"""
    overallMean = np.mean(X, axis=0)
    labels = np.unique(y)
    n_features = X.shape[1]
    Sb = np.zeros((n_features, n_features))
    for label in labels:
        XOfLabel = X[y == label]
        n_samplesOfLabel = XOfLabel.shape[0]
        meanOfClass = np.mean(XOfLabel, axis=0)
        meanDiff = (meanOfClass - overallMean).reshape(n_features, 1)
        Sb += n_samplesOfLabel * (meanDiff @ meanDiff.T)
    return Sb


def solve_fisher_direction(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    """Solve for Fisher's Linear Discriminant Direction"""
    Sw = get_Sw(X, y)
    Sb = get_Sb(X, y)

    # Solve the generalized eigenvalue problem equation Sb*w = lambda*Sw*w
    eigvals, eigvecs = scipy.linalg.eigh(Sb, Sw)
    maxEigIndex = np.argmax(eigvals)
    w = eigvecs[:, maxEigIndex]
    return w


def split_data(X: np.ndarray, y: np.ndarray, train_ratio: float = 0.8):
    """Split data into training and testing sets"""
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    np.random.shuffle(indices)

    train_size = int(n_samples * train_ratio)
    train_indices = indices[:train_size]
    test_indices = indices[train_size:]

    X_train = X[train_indices]
    y_train = y[train_indices]
    X_test = X[test_indices]
    y_test = y[test_indices]

    return X_train, y_train, X_test, y_test


def train_fisher_classifier(X: np.ndarray, y: np.ndarray):
    """Train Fisher Classifier"""
    w = solve_fisher_direction(X, y)
    projections = X @ w  # compute projection
    labels = np.unique(y)
    meanProjections = []
    for label in labels:
        meanProjections.append((label,np.mean(projections[y == label])))
    sortedMeans = sorted(meanProjections,key=lambda x: x[1])
    thresholds=[]
    for i in range(len(sortedMeans)-1):
        meanPrev=sortedMeans[i][1]
        meanPost=sortedMeans[i+1][1]
        thresholds.append((meanPrev+meanPost)/2)

    return w,thresholds,sortedMeans


def predict_fisher_classifier(X: np.ndarray, w: np.ndarray, thresholds: list,sortedMeans:List[Tuple[int,float]]) -> np.ndarray:
    """Predict using Fisher Classifier"""
    projections = X @ w
    y_pred = np.zeros(projections.shape[0])
    labels = [item[0] for item in sortedMeans]
    for i, projection in enumerate(projections):
        for j, threshold in enumerate(thresholds):
            if projection < threshold:
                y_pred[i] = labels[j]
                break
        else:
            y_pred[i] = labels[-1]
    return y_pred

def evaluate_classifier(y_true: np.ndarray, y_pred: np.ndarray):
    """Evaluate Classifier Accuracy"""
    accuracy = np.mean(y_true == y_pred)
    return accuracy




if __name__ == "__main__":
    np.random.seed(42)
    
    data = load_breast_cancer()
    X = data["data"]
    y = data["target"]

    X_train, y_train, X_test, y_test = split_data(X, y)

    w,thresholds,sortedMeans = train_fisher_classifier(X_train, y_train)

    y_pred = predict_fisher_classifier(X_test, w, thresholds,sortedMeans)

    accuracy = evaluate_classifier(y_test, y_pred)
    print(f"Fisher Classifier Accuracy: {accuracy * 100:.2f}%")
\end{lstlisting}
\clearpage
\chapter{第四次上机实验}
\section{实验要求}
基于朴素贝叶斯的犯罪类型预测

1. 给定LA犯罪数据集：train.csv，test.csv 共44948个训练样本，14983个测试样本

2. 根据某案件的“案发时间”“所属警区”“案发地点”等属性，预测案件的类型，例如“抢劫”“偷窃”“杀人”等。

3. 属性均为离散的

4. 使用Python编程实现，完成基于朴素贝叶斯的对犯罪类型的预测

\section{数据分析与处理}
在数据预处理阶段，首先加载了训练集和测试集数据。
数据包含5个属性：
“Dates”（日期）、“Category”（犯罪类型）、“Descript”（描述）、
“DayOfWeek”（星期几）、“PdDistrict”（警区）。
接下来，将对这些属性进行适当的处理，确保数据适用于朴素贝叶斯分类器。

\section{实验步骤与原理}

\subsection{贝叶斯分类器原理}
贝叶斯分类器基于贝叶斯定理，后验概率由先验概率和类条件概率密度计算得出。对于二分类问题，贝叶斯公式为：
\[
P(\theta|X) = \frac{P(X|\theta) P(\theta)}{P(X)}
\]
其中$P(\theta)$为先验，$P(\theta|X)$为后验。我们的目标是通过最大化后验概率来进行分类。

\subsection{分类规则}
在二分类问题中，我们使用后验概率进行决策：
\[
P(\omega_1|X) > P(\omega_2|X) \quad \Rightarrow \quad \text{选择} \ \omega_1
\]
条件错误概率为：
\[
P(\text{error} \mid x, \text{decide} \ \omega_1) = P(\omega_2 \mid x)
\]
根据错误概率最小化原则，选择后验概率大的类别进行分类。

\subsection{朴素贝叶斯分类器}
朴素贝叶斯分类器是一种基于贝叶斯定理的分类方法，假设特征条件独立。在多分类问题中，对于每个类别$C_i$，朴素贝叶斯分类器通过以下公式计算后验概率：
\[
P(C_i | X) = \frac{P(X | C_i) P(C_i)}{P(X)}
\]
由于特征条件独立，$P(X | C_i)$可以分解为各个特征条件概率的乘积：
\[
P(X | C_i) = \prod_{j=1}^{n} P(x_j | C_i)
\]
因此，朴素贝叶斯分类器的决策规则是选择使后验概率最大的类别：
\[
\hat{C}(X) = \arg \max_{C_i} P(X | C_i) P(C_i)
\]

\subsection{实验中的朴素贝叶斯方法}
本实验使用的犯罪数据集中的特征（如案发时间、警区等）均为离散型数据，这与传统的连续型数据不同。在这种情况下，朴素贝叶斯分类器的特征条件概率$P(x_j | C_i)$是通过每个特征值在各类别中的频率来估计的，即：
\[
P(x_j | C_i) = \frac{\text{count}(x_j, C_i)}{\text{count}(C_i)}
\]
其中$\text{count}(x_j, C_i)$表示类别$C_i$中出现特征值$x_j$的次数，而$\text{count}(C_i)$是类别$C_i$中样本的总数。

因此，本实验中朴素贝叶斯的实现利用了训练数据集的频率信息来计算每个特征在各类别下的条件概率，并使用这些条件概率来预测测试集中的犯罪类型。

\section{实验结论与分析}

在本实验中，我们使用朴素贝叶斯分类器对犯罪数据集进行了分类，结果如下：

\subsection{评估指标}
模型在测试集上的评估结果如下：
\begin{itemize}
    \item 准确率 (Accuracy): 0.9997
    \item 精确度 (Precision, macro): 0.9978
    \item 召回率 (Recall, macro): 0.9972
    \item F1-得分 (F1-score, macro): 0.9975
\end{itemize}

这些评估指标表明，模型在测试集上表现非常好，准确率接近1，且精确度、召回率和F1得分都非常高，表明模型在分类时能够有效地平衡假阳性和假阴性。

\subsection{混淆矩阵}
混淆矩阵如下所示：
\[
\begin{bmatrix}
13600 & 1 & 2 & 0 \\
0 & 948 & 0 & 0 \\
1 & 0 & 297 & 0 \\
0 & 1 & 0 & 133 \\
\end{bmatrix}
\]

从混淆矩阵可以看出，模型在大多数类别中正确分类了样本。具体分析如下：
\begin{itemize}
    \item 类别 1 中有13600个样本被正确分类，仅有少数几个误分类为类别 2 和 3。
    \item 类别 2 和类别 3 的分类效果也非常好，误分类的样本数量极少。
    \item 类别 4 的分类效果稍差，但仍然具有较高的准确率，只有少数样本被误分类为类别 2。
\end{itemize}

\subsection{分析与讨论}
从实验结果来看，朴素贝叶斯分类器在本次犯罪预测任务中表现出色。特别是针对离散型特征数据，模型能够很好地捕捉不同类别之间的关系并做出准确的预测。然而，虽然总体表现很好，但我们仍然注意到在类别 4 上存在一些误分类现象，未来可以通过更复杂的特征工程或尝试其他分类算法来进一步提高模型的表现。
\section{实验代码}
\begin{lstlisting}[language=Python]
import csv
from collections import defaultdict
import math

from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
)


class DiscreteNB:
    def __init__(self, alpha=1.0):
        self.alpha = alpha  # 平滑参数
        self.class_prior = {}  # P(y)
        self.feature_cond_prob = {}  # P(x_i=v | y)
        self.label_encoders = {}  # 文本特征映射表
        self.classes = []
        self.feature_values = {}  # 每个特征可能出现的取值

    # 文本离散特征编码器
    def fit_label_encoders(self, X):
        # X: list of samples, each is a list of string features
        n_features = len(X[0])
        for i in range(n_features):
            self.label_encoders[i] = {}
            self.feature_values[i] = set()

        for x in X:
            for i, v in enumerate(x):
                if v not in self.label_encoders[i]:
                    self.label_encoders[i][v] = len(self.label_encoders[i])
                self.feature_values[i].add(v)
        for i in range(n_features):
            self.label_encoders[i]["__UNK__"] = -1

    def encode(self, X):
        X_enc = []
        for x in X:
            row = []
            for i, v in enumerate(x):
                row.append(self.label_encoders[i][v])
            X_enc.append(row)
        return X_enc

    def fit(self, X, y):
        # 拟合文本编码器
        self.fit_label_encoders(X)
        X = self.encode(X)

        self.classes = list(set(y))
        n_samples = len(X)
        n_features = len(X[0])

        # 类别计数
        class_count = defaultdict(int)
        # 每个类别、每个特征维度、每个可能取值的计数
        feature_count = defaultdict(
            lambda: [defaultdict(int) for _ in range(n_features)]
        )

        # 统计频数
        for x_row, label in zip(X, y):
            class_count[label] += 1
            for i, v in enumerate(x_row):
                feature_count[label][i][v] += 1

        # 计算先验 P(y)
        for c in self.classes:
            self.class_prior[c] = (class_count[c] + self.alpha) / (
                n_samples + self.alpha * len(self.classes)
            )

        # 计算条件概率 P(x_i=v | y)
        self.feature_cond_prob = defaultdict(list)
        for c in self.classes:
            for i in range(n_features):
                cond = {}
                total_count = sum(feature_count[c][i].values())
                V = len(self.feature_values[i])
                for v_enc in feature_count[c][i]:
                    cond[v_enc] = (feature_count[c][i][v_enc] + self.alpha) / (
                        total_count + self.alpha * V
                    )
                # 对未出现的取值做平滑
                for txt_val, v_enc in self.label_encoders[i].items():
                    if v_enc not in cond:
                        cond[v_enc] = self.alpha / (total_count + self.alpha * V)
                self.feature_cond_prob[c].append(cond)

    # 预测单条
    def predict_one(self, x):
        # 文本转编码
        x = [
            self.label_encoders[i].get(v, -1)  # 未见过的特征值 → -1
            for i, v in enumerate(x)
        ]
        best_class = None
        best_logp = -1e18

        for c in self.classes:
            logp = math.log(self.class_prior[c])
            for i, v_enc in enumerate(x):
                logp += math.log(self.feature_cond_prob[c][i][v_enc])
            if logp > best_logp:
                best_logp = logp
                best_class = c

        return best_class

    # 批量预测
    def predict(self, X):
        return [self.predict_one(x) for x in X]


# 数据加载
def load_data(path):
    X = []
    y = []
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # 选择部分离散特征
            # 你可按需要修改
            feature_row = [
                row["DayOfWeek"],
                row["PdDistrict"],
                row["Descript"],
            ]
            X.append(feature_row)
            y.append(row["Category"])
    return X, y


# 评估
def evaluate(y_true, y_pred):
    print("====== Evaluation ======")

    acc = accuracy_score(y_true, y_pred)
    print(f"Accuracy: {acc:.4f}")

    # precision, recall, f1 for each class (macro means均值)
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )

    print(f"Precision (macro): {precision:.4f}")
    print(f"Recall (macro):    {recall:.4f}")
    print(f"F1-score (macro):  {f1:.4f}")

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_true, y_pred)
    print(cm)


if __name__ == "__main__":
    X_train, y_train = load_data("./EX4/data/train.csv")
    X_test, y_test = load_data("./EX4/data/test.csv")

    nb = DiscreteNB(alpha=1.0)
    nb.fit(X_train, y_train)
    y_pred = nb.predict(X_test)

    evaluate(y_test, y_pred)
\end{lstlisting}
\clearpage
\chapter{第五次上机实验}
\section{实验要求}
实验五、基于决策树的犯罪类型预测

1. 给定LA犯罪数据集：train.csv，test.csv 共44948个训练样本，14983个测试样本

2. 根据某案件的“案发时间”“所属警区”“案发地点”等属性，预测案件的类型，例如“抢劫”“偷窃”“杀人”等。

3. 属性均为离散的

4. 使用决策树方法进行犯罪类型预测，Python编程实现，可以直接调包

5. 看懂代码，找到前3个进行分支的属性

\section{数据分析与处理}
数据仍为洛杉矶犯罪数据，与实验四相同。

\section{实验步骤与原理}

\subsection{决策树原理}
决策树是一类基于树结构的监督学习模型，广泛应用于分类和回归任务。在分类问题中，决策树通过一系列条件划分（if–else规则）将样本逐步划分到不同的叶节点，并在叶节点给出分类结果。每个内部节点表示特征空间中的一个维度，叶节点则对应最终的分类标签。决策树的目标是通过不断选择最佳特征对样本进行划分，最终形成一颗能够有效分类的新样本的树。

\subsection{Hunt算法}
Hunt算法是生成决策树的通用框架。它通过以下规则决定一个节点是否为叶子节点，或是否需要继续分裂为内部节点：

- \textbf{叶子节点}：
  \begin{itemize}
      \item 当前节点下所有数据标签相同（样本纯），则该节点为叶子节点，输出标签为这些样本的标签。
      \item 当前节点下样本标签不同（样本非纯），但样本不可再分裂时，该节点为叶子节点，输出标签为该节点中多数样本的标签：
      \begin{itemize}
          \item 所有特征在当前路径上已被使用过。
          \item 当前节点的所有样本在所有特征上取值完全相同。
          \item 样本数量小于预设的最小分裂样本数。
          \item 所有特征的划分信息增益为0。
      \end{itemize}
  \end{itemize}
- \textbf{内部节点}：若样本标签不同，且样本数大于最小分裂样本数，且存在具有信息增益的可用特征，则选择信息增益最大的特征，并找到该特征的最佳划分点，继续向下分裂。

\subsection{节点分裂规则}
在决策树的节点分裂过程中，首先需要确定如何量化每个节点的纯度。常用的指标包括熵和信息增益。

\subsubsection{熵}
熵用于衡量一个随机变量的不确定性。对于离散型随机变量$X$，其熵定义为：
\[
H(X) = -\sum_{i=1}^k P(X=k) \log P(X=k)
\]
对于连续型随机变量$X$，熵的定义为：
\[
H(X) = -\int f_X(x) \log f_X(x) dx
\]
其中$P(X=k)$是$X$取值为$k$的概率，$f_X(x)$是$X$的概率密度函数。

\subsubsection{节点样本纯度}
节点样本标签的熵用于衡量该节点的纯度。样本纯度越高，熵越低。我们通常使用熵来定义不纯度：
\[
H(D) = -\sum_{i=1}^m P(X=k) \log P(X=k)
\]
其中$D$是一个节点中的样本集合，$m$是样本类别数。

\subsubsection{信息增益}
信息增益衡量的是通过特征$X$划分数据集$D$后，样本集的不确定性（熵）的减少程度。设$D_1, D_2, \dots, D_k$为划分后的子节点，$|D|$为样本集合$D$的大小，则划分后的加权平均熵为：
\[
H(D|X) = \sum_{i=1}^k \frac{|D_i|}{|D|} H(D_i)
\]
信息增益定义为：
\[
\text{Gain}(D, X) = H(D) - H(D|X)
\]
信息增益越大，说明通过特征$X$进行划分能够有效减少节点的熵，从而提高分类效果。

\subsection{实验中的决策树使用方法}
在本实验中，我们使用决策树算法对犯罪数据进行分类。数据集中的特征均为离散型，因此我们通过计算每个特征的信息增益，选择信息增益最大的特征进行划分。具体步骤包括：
\begin{itemize}
    \item 计算每个特征的熵和信息增益。
    \item 选择信息增益最大的特征作为当前节点的划分依据。
    \item 对每个子节点递归地重复此过程，直到满足停止条件（例如：样本纯度为100\%或者达到最小分裂样本数）。
\end{itemize}
决策树最终输出的是每个叶节点对应的犯罪类型，通过这种方式，模型能够基于训练集中的特征和标签学习到如何分类新的犯罪样本。

\subsubsection{ID3算法}
ID3（Iterative Dichotomiser 3）算法使用**信息增益**来选择最佳特征进行节点分裂。在每个节点，ID3算法计算所有特征的信息增益，并选择信息增益最大的特征作为划分依据。该算法适用于离散型特征，并且在每次分裂时会选择最能减少熵的特征。

\subsubsection{C4.5算法}
C4.5是ID3的改进版本，主要区别在于：
\begin{itemize}
    \item C4.5使用**增益比**（Gain Ratio）代替信息增益，避免了ID3偏好选择取值较多的特征的问题。
    \item C4.5能够处理**连续型特征**，通过选择一个合适的划分点将连续特征离散化。
    \item C4.5引入了剪枝技术，通过减少树的复杂度来提高模型的泛化能力。
\end{itemize}
C4.5在信息增益的基础上，通过计算每个特征的增益比来避免偏向选择某些特征，使得算法在处理实际问题时更加稳定和有效。

\section{实验结论与分析}

在本实验中，我们使用决策树对犯罪数据集进行了分类，结果如下：

\subsection{评估指标}
模型在测试集上的评估结果为：
\begin{itemize}
    \item 准确率 (Accuracy): 0.9254
    \item 精确度 (Precision, macro): 0.6611
    \item 召回率 (Recall, macro): 0.5843
    \item F1得分 (F1-score, macro): 0.6144
\end{itemize}

\subsection{混淆矩阵}
混淆矩阵如下所示：
\[
\begin{bmatrix}
13556 & 0 & 33 & 14 \\
948 & 0 & 0 & 0 \\
63 & 0 & 235 & 0 \\
60 & 0 & 0 & 74 \\
\end{bmatrix}
\]

\subsection{分析与讨论}
尽管模型在准确率方面表现较好（92.54\%），但精确度和召回率较低，特别是对于某些类别的预测效果较差。混淆矩阵表明，类别 1 和类别 4 的样本被正确分类的比例较高，而类别 2 和 3 的样本则被较多地误分类为其他类别。该结果表明决策树对于某些类别的分类能力较弱，可能需要通过特征工程或改进模型（如使用剪枝）来提高性能。

\section{实验代码}
\begin{lstlisting}[language=Python]
import csv
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
)
from sklearn.tree import DecisionTreeClassifier

def load_data(path):
    X = []
    y = []
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # 选择部分离散特征
            # 你可按需要修改
            feature_row = [
                row["DayOfWeek"],
                row["PdDistrict"],
                row["Descript"],
            ]
            X.append(feature_row)
            y.append(row["Category"])
    return X, y

def encode_categorical_features(X, y):
    # 转置：列处理
    X = np.array(X)
    n_features = X.shape[1]

    encoders = []
    X_encoded = np.empty_like(X, dtype=int)

    # 对每列进行 LabelEncoder
    for i in range(n_features):
        le = LabelEncoder()
        X_encoded[:, i] = le.fit_transform(X[:, i])
        encoders.append(le)

    # 对 y 编码
    le_y = LabelEncoder()
    y_encoded = le_y.fit_transform(y)

    return X_encoded, y_encoded, encoders, le_y

def load_and_encode(path: str):
    X, y = load_data(path)
    return encode_categorical_features(X, y)

# 评估
def evaluate(y_true, y_pred):
    print("====== Evaluation ======")

    acc = accuracy_score(y_true, y_pred)
    print(f"Accuracy: {acc:.4f}")

    # precision, recall, f1 for each class (macro means均值)
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )

    print(f"Precision (macro): {precision:.4f}")
    print(f"Recall (macro):    {recall:.4f}")
    print(f"F1-score (macro):  {f1:.4f}")

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_true, y_pred)
    print(cm)


if __name__ == "__main__":
    X_train, y_train, _, _ = load_and_encode("./EX5/data/train.csv")
    X_test, y_test, _, _ = load_and_encode("./EX5/data/test.csv")

    clf = DecisionTreeClassifier(max_depth=4)
    clf.fit(X=X_train, y=y_train)
    y_pred = clf.predict(X=X_test)
    evaluate(y_test, y_pred)
\end{lstlisting}
\printbibliography

\end{document}
