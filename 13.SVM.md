## 模型描述
SVM是一种针对**线性可分**数据的**有监督**分类算法。
设训练样本$D=\{(x_i,y_i)\}_{i=1}^m,y_i \in \{-1,+1\}$,SVM可以在特征向量$x$的特征空间内，找到一个超平面，把正类和负类样本分开。
设划分超平面为
$$
w^Tx+b=0
$$
简记为超平面$(w,b)$

## 函数间隔与几何间隔

由解析几何知识可知，样本空间中任意点$x$到超平面的距离为
$$
r=\frac{|w^Tx+b|}{||w||}
$$

单个样本点$(x_i,y_i)$到超平面的两个间隔

**函数间隔**定义为
$$
\hat{\gamma_i}=y_i(w^Tx_i+b)
$$


**几何间隔**定义为
$$
\gamma_i=\frac{y_i(w^Tx_i+b)}{||w||}=\frac{\hat{\gamma_i}}{||w||}
$$
 记$\hat{\gamma}=\min\hat{\gamma_i}$为整个数据集到超平面的函数间隔 记$\gamma=\min\gamma_i$为整个数据集到超平面的几何间隔 *当$||w||=1$时，函数间隔与几何间隔相等；如果$w,b$等比例缩放，则函数间隔等比例缩放，几何间隔不变* ## 支持向量 称正类样本和负类样本中到超平面距离最近的各个向量为**支持向量**， 从正类和负类的支持向量中分别任取一个，分别记为$x_{+},x_{-}$ ， 由于对$(w,b)$等比例缩放不改变对应的超平面，我们增加一个约束 $$ w^Tx_{+}+b=1,w^Tx_{-}+b=-1 $$ 则在此约束之下，正类、负类到超平面的距离之和为 $$ M=\frac{2}{||w||} $$
## 最大间隔分离超平面

SVM的目标是求得一个**几何间隔最大**的分离超平面
$$
max\ M \quad s.t.\ y_i(w^Tx_i+b)\geq 1 
$$ 

值得注意的是，上述约束优化中的约束条件**没有显式给出"支持向量函数值为正负1"的约束**，但实际上**这个约束是满足的**，理由如下： 为了满足“支持向量函数值为正负1"的约束，需要避免的就是对$(w,b)$的等比例缩放， 在$y_i(w^Tx_i+b)\geq 1$约束下，我们只能对$(w,b)$等比例放大，不能缩小，而等比例放大又会导致M的值变小，于是在最大化M的目标下，求出的解会满足“支持向量函数值为正负1”的约束条件。 此约束优化问题等价于 
$$ 
min \ \frac{1}{2}||w||^2 \quad s.t.\ y_i(w^Tx_i+b)\geq 1 
$$

## 约束优化求解（硬间隔 SVM）

我们考虑线性可分情况下的硬间隔支持向量机，其原始优化问题为：

$$
\min_{\omega,b}\ \frac{1}{2}\|\omega\|^2
\quad
\text{s.t.}\quad
y_i(\omega^Tx_i+b)\ge 1,\ i=1,\dots,m
$$

### 构造拉格朗日函数

对每一个不等式约束
$$
y_i(\omega^Tx_i+b)\ge 1
$$
引入拉格朗日乘子 $\lambda_i\ge 0$，构造拉格朗日函数：

$$
L(\omega,b,\lambda)
=
\frac{1}{2}\|\omega\|^2
+
\sum_{i=1}^m
\lambda_i\bigl(1-y_i(\omega^Tx_i+b)\bigr)
$$

其中：
- $\omega\in\mathbb{R}^d$ 为法向量
- $b\in\mathbb{R}$ 为偏置
- $\lambda_i\ge 0$ 为拉格朗日乘子

### 转化为对偶问题

对偶问题通过对拉格朗日函数关于原变量 $\omega,b$ 求极小得到。

#### 一阶驻点条件（Stationarity）

对 $\omega$ 求偏导并令其为零：

$$
\frac{\partial L}{\partial \omega}
=
\omega
-
\sum_{i=1}^m \lambda_i y_i x_i
=
0
$$

得到：
$$
\omega
=
\sum_{i=1}^m \lambda_i y_i x_i
$$

对 $b$ 求偏导并令其为零：

$$
\frac{\partial L}{\partial b}
=
-
\sum_{i=1}^m \lambda_i y_i
=
0
$$

得到等式约束：
$$
\sum_{i=1}^m \lambda_i y_i = 0
$$

#### 构造对偶目标函数

将 $\omega=\sum_i \lambda_i y_i x_i$ 代回拉格朗日函数，消去 $\omega,b$，得到仅关于 $\lambda$ 的函数：

$$
D(\lambda)
=
\sum_{i=1}^m \lambda_i
-
\frac{1}{2}
\sum_{i=1}^m\sum_{j=1}^m
\lambda_i\lambda_j y_i y_j x_i^T x_j
$$

于是得到 **对偶优化问题**：

$$
\max_{\lambda}\ 
\sum_{i=1}^m \lambda_i
-
\frac{1}{2}
\sum_{i=1}^m\sum_{j=1}^m
\lambda_i\lambda_j y_i y_j x_i^T x_j
$$

$$
\text{s.t.}\quad
\lambda_i\ge 0,\quad
\sum_{i=1}^m \lambda_i y_i = 0
$$

这是一个凸二次规划问题。

### 求解 $\lambda$

对偶问题在小样本下可以直接求驻点；
$$
\forall i,\quad \frac{\partial D(\lambda)}{\partial \lambda_i}=1-\sum_{j=1}^m\lambda_jy_iy_jx_i^Tx_j=0
$$
大样本下一般不存在解析解，需要使用数值优化方法（如 QP 或 SMO 算法）求解。

从 KKT 条件中的 **互补松弛条件**：

$$
\lambda_i\big(y_i(\omega^Tx_i+b)-1\big)=0
$$

可以得到：

- 若 $\lambda_i>0$，则
  $$
  y_i(\omega^Tx_i+b)=1
  $$
  样本 $x_i$ 位于间隔边界上，是 **支持向量**

- 若 $\lambda_i=0$，则
  $$
  y_i(\omega^Tx_i+b)>1
  $$
  样本位于间隔之外，不是支持向量

### 回代求解 $\omega$ 和 $b$

当最优 $\lambda^*$ 求得后：

$$
\omega
=
\sum_{i=1}^m \lambda_i^* y_i x_i
$$

对任一支持向量 $x_k$（满足 $\lambda_k^*>0$），由
$$
y_k(\omega^Tx_k+b)=1
$$
可解得偏置 $b$：

$$
b
=
y_k-\omega^Tx_k
$$

## 约束优化（软间隔 SVM）

当训练样本 **线性不可分** 时，引入松弛变量 $\xi_i$，允许部分样本违反间隔约束，从而得到软间隔支持向量机。

### 原始优化问题（Primal）

$$
\min_{w,b,\xi}
\quad
\frac{1}{2}\|w\|^2 + C\sum_{i=1}^N \xi_i
$$

$$
\text{s.t.}\quad
y_i(w^Tx_i+b) \ge 1-\xi_i,\quad i=1,\dots,N
$$

$$
\xi_i \ge 0,\quad i=1,\dots,N
$$

其中：
- $\xi_i$ 为松弛变量，表示样本 $x_i$ 对间隔约束的违反程度
- $C>0$ 为惩罚参数，用于平衡“最大间隔”和“误分类惩罚”


### 松弛变量的几何含义

- $\xi_i=0$：样本被正确分类，且在间隔之外或边界上
- $0<\xi_i<1$：样本被正确分类，但落在间隔内部
- $\xi_i\ge 1$：样本被误分类


### 构造拉格朗日函数

对不等式约束分别引入拉格朗日乘子：
- $\alpha_i\ge 0$ 对应 $y_i(w^Tx_i+b)\ge 1-\xi_i$
- $\mu_i\ge 0$ 对应 $\xi_i\ge 0$

构造拉格朗日函数：

$$
L(w,b,\xi,\alpha,\mu)
=
\frac{1}{2}\|w\|^2
+
C\sum_{i=1}^N \xi_i
-
\sum_{i=1}^N \alpha_i\big(y_i(w^Tx_i+b)-1+\xi_i\big)
-
\sum_{i=1}^N \mu_i\xi_i
$$


### KKT 一阶驻点条件

对 $w,b,\xi_i$ 求偏导并令其为零。

#### 对 $w$

$$
\nabla_w L
=
w-\sum_{i=1}^N \alpha_i y_i x_i
=
0
$$

$$
\Rightarrow
w=\sum_{i=1}^N \alpha_i y_i x_i
$$

#### 对 $b$

$$
\nabla_b L
=
-\sum_{i=1}^N \alpha_i y_i
=
0
$$

$$
\Rightarrow
\sum_{i=1}^N \alpha_i y_i = 0
$$

#### 对 $\xi_i$

$$
\nabla_{\xi_i} L
=
C-\alpha_i-\mu_i
=
0
$$

$$
\Rightarrow
\alpha_i + \mu_i = C
$$

结合 $\alpha_i\ge 0,\ \mu_i\ge 0$，得到：

$$
0 \le \alpha_i \le C
$$

### 转化为对偶问题（Dual）

将 $w=\sum_i\alpha_i y_i x_i$ 代回拉格朗日函数，并消去 $w,b,\xi,\mu$，得到仅关于 $\alpha$ 的对偶问题：

$$
\max_{\alpha}
\quad
\sum_{i=1}^N \alpha_i
-
\frac{1}{2}
\sum_{i=1}^N\sum_{j=1}^N
\alpha_i\alpha_j y_i y_j (x_i^T x_j)
$$

$$
\text{s.t.}\quad
\sum_{i=1}^N \alpha_i y_i = 0
$$

$$
0 \le \alpha_i \le C,\quad i=1,\dots,N
$$

这是一个 **凸二次规划问题**。

### 支持向量的判别（由 KKT 条件）

由互补松弛条件：

$$
\alpha_i\big(y_i(w^Tx_i+b)-1+\xi_i\big)=0
$$

$$
\mu_i\xi_i=0
$$

可得：

- $0<\alpha_i<C$  
  样本位于间隔边界上，是支持向量

- $\alpha_i=C$  
  样本违反间隔约束（可能被误分类）

- $\alpha_i=0$  
  样本在间隔之外，不是支持向量

### 回代求解 $w$ 和 $b$

当最优 $\alpha^*$ 求得后：

$$
w^*
=
\sum_{i=1}^N \alpha_i^* y_i x_i
$$

对任一满足 $0<\alpha_k^*<C$ 的样本，由

$$
y_k(w^{*T}x_k+b^*)=1-\xi_k
$$

可得偏置：

$$
b^*
=
y_k-w^{*T}x_k
$$

通常取多个支持向量的平均值。

### 分类决策函数

$$
f(x)=\text{sign}(w^{*T}x+b^*)
$$
