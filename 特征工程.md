## 基本概念

## 特征提取
在统计中降维往往以保持原有信息为目的，而机器学习的降维是和人物高度相关的，不一定要保持悠悠的信息。

### PCA
给定d个原始特征，经过正交线性变换得到一组按照“重要性”大小排序的特征，取前m个特征。
其中重要性定义为数据在该维度上的方差，方差越大越重要。

假设原始特征矩阵为$X^{n \times d}$,其中n为样本数、d为原始数据的维度. 
关键是找到一组投影向量，使得原始数据在这些方向上的投影的方差比较大，投影方向上数据方差越大说明该方向越重要，把所有的投影方向按照数据投影的方差大小排序，选出前m个方向向量就得到了这组数据的主成分。

中心化：$\tilde{X}=X-\bar{X}$
把每个特征当成一个随机变量，计算特征之间的协方差矩阵$\Sigma = \frac{1}{n-1}\tilde{X}^T\tilde{X}$
*除以n-1计算的是样本协方差，保证无偏估计*

对任意单位方向向量$\omega$ ,数据在该方向上的投影的一写成$z_i = \omega^T \tilde{X}$ 
在该方向上的样本方差可以写为
$$
\begin{aligned}
Var(z) & = \frac{1}{n-1} \sum_i(z_i-\bar{z})^2\\
       & = \frac{1}{n-1} \sum_iz_i^2 \\
       & = \frac{1}{n-1} \sum_i(\omega^T\tilde{x_i})^2 \\
       & = \omega^T(\frac{1}{n-1}\tilde{X}^T\tilde{X})\omega \\
       & = \omega^T\Sigma\omega
\end{aligned}
$$
首先要找PCA的第一个维度及方差最大的方向向量，这就转化为一个约束优化问题：
$$
	\max_\omega \ \omega^T\Sigma\omega \quad s.t.\quad \omega^T\omega=1
$$
使用拉格朗日乘子法：
$$
$$
$$
\begin{aligned}

L(\omega,\lambda) &= \omega^T\Sigma\omega-\lambda\omega^T\omega \\
\\
\frac{\partial{L}}{\partial{\omega}} 
&= 2\Sigma\omega-2\lambda\omega \\
&= 0 \\
\end{aligned}
$$
$\Sigma\omega-\lambda\omega=0$求解这个方程就是求$\Sigma$的特征值和特征向量。


