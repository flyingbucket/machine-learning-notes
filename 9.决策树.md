**决策树**是一类基于**树结构**的监督学习模型，可用于**分类**和**回归**任务。  

它通过对特征空间进行一系列**条件划分（if–else 规则）**，将样本逐步划分到不同的叶节点，并在叶节点给出预测结果。

下面我们都以分类问题为背景来展开。

决策树的每个叶子节点对应一个分类标签，每个内部节点(包含根节点)对应特征的一个维度；分类标签和特征维度都可以分别被多个叶子节点和内部节点来使用。

决策树的每一个内部节点都会把该节点上的样本分成两部分送到后续的节点，其中根节点的样本就是所有的训练数据。
## Hunt算法

Hunt算法是生成决策树的通用框架，规定了及诶但分裂的基础规则。
Hunt算法判断一个节点是可继续分裂的内部节点还是叶子节点的规则如下：

- 叶子节点：
	- 当前节点下所有数据标签相同（样本纯）,此时以这些样本的标签为该叶节点的输出标签
	- 当前节点下样本标签不同（样本非纯），但样本不可再分裂，此时以该节点上多数样本的标签为输出标签：
		- 所有特征在当前路径上都已经被使用过
		- 当前节点所有样本在所有特征上取值完全相同，及几个不同标签样本在特征空间中坍缩为一个点
		- 样本数量少于超参数所规定的最小分裂样本数
		- 所有特征的划分信息增益都为0

 - 内部节点：若样本标签不同（样本非纯），样本书大于最小分裂样本书且存在具有信息增益的可用特征，则选择一个信息增益最大的特征并找到该特征的划分方式，继续向下分裂。

## 节点分裂规则

### 熵
熵用于衡量一个随机变量的不确定性
随机变量X的熵定义为，$-log(p(X)$的期望

对于离散型随机变量X：
$$
h(X)=-\sum_{i=1}^kP(X=k)log(P(X=k))
$$
对于连续型随机变量X：
$$
h(X)=-\int f_X(x)logf_X(x)dx
$$
### 节点样本纯度
节点样本标签属于离散型随机变量，
一个节点上样本的纯度，使用其标签的熵来衡量。
我们一般用熵定义*不纯度*即可
$$
H(D)=-\sum_{i=1}^mP(X=k)log(P(X=k))
$$

### 信息增益
选定一个特征$X$和该特征上的某一点进行划分，设划分后的子节点为$D_1,\cdots,D_k$
用$|D|$表示一个样本集合中样本的数量
则划分后的**加权平均熵**为
$$
H(D|X)=\sum_{i=1}^k\frac{|D_i|}{|D|}H(D_i)
$$
信息增益定义为
$$
Gain(D,X)=H(D)-H(D|X)
$$
信息增益表示的是划分后子节点的熵的减少情况，熵的减少幅度越大说明这一次划分对最终分类给出的*信息量的提升幅度*越大。

### 分裂